{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "The notebook presents the execution of KONG for the Dunnhumby and MovieLens datasets.\n",
    "\n",
    "The _MovieLens dataset_ consists of movie ratings given by users. Selecting a given user, we connect it wo all movies s/he has rated, thus we build a bipartite graph. Movies are labeled by one of 19 genres and users by their quantized age. We create graphs by traversing the 2-hop neighborhood for each user, i.e., each graph is rooted at a given user. The class of the graph is the user's gender.\n",
    "\n",
    "The _Dunnhumby dataset_ represents customers and purchased products. Customers are labeled by their affluence and products to one of the 9 categories they belong to. Similarly to MovieLens, we create a graph starting from each user and traversing its 2-hop neighborhood. The class of each graph is the customer's lifestage which is a combination of age and gender. \n",
    "\n",
    "Before running the code please do the following steps: \n",
    "* Download 10 files with random numbers from [here](https://web.archive.org/web/20160119150146/http://stat.fsu.edu/pub/diehard/cdrom/) and store them into the folder **random** (if there is no such folder, create it).\n",
    "* For the MovieLens experiments download the MovieLens files from [here](http://files.grouplens.org/datasets/movielens/ml-1m.zip) and decompress it into the folder **data**.\n",
    "* Make sure the folder **graphs** is empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we first create graphs from the Dunnhumby/MovieLens dataset\n",
    "import read_dunnhumby\n",
    "import movielens_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the path is correct \n",
    "dataset = 'data/dunnhumby/merged.csv'\n",
    "\n",
    "#this is the directory where we write the generated graphs, if not existent, create it\n",
    "graph_folder = 'graphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SHOP_WEEK', 'SHOP_DATE', 'SHOP_WEEKDAY', 'SHOP_HOUR', 'QUANTITY',\n",
      "       'SPEND', 'PROD_CODE', 'PROD_CODE_10', 'PROD_CODE_20', 'PROD_CODE_30',\n",
      "       'PROD_CODE_40', 'CUST_CODE', 'CUST_PRICE_SENSITIVITY', 'CUST_LIFESTAGE',\n",
      "       'BASKET_ID', 'BASKET_SIZE', 'BASKET_PRICE_SENSITIVITY', 'BASKET_TYPE',\n",
      "       'BASKET_DOMINANT_MISSION', 'STORE_CODE', 'STORE_FORMAT',\n",
      "       'STORE_REGION'],\n",
      "      dtype='object')\n",
      "1566\n",
      "3101\n",
      "1566\n",
      "12905\n"
     ]
    }
   ],
   "source": [
    "#convert the dataset to graphs\n",
    "read_dunnhumby.to_graphs(dataset, graph_folder)\n",
    "\n",
    "#For the MovieLens dataset\n",
    "#the last argument is the number of graphs we want to generate\n",
    "#nr_graphs = 1000\n",
    "#movielens_to_graph.create_graphs(E, users, items, graph_folder, nr_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary functions for reading and writing the graphs and the generated explicit feature maps \n",
    "import read_write_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we read the generated graphs into data structures. \n",
    "Vs, Es, classes = read_write_utilities.read_dh_format('graphs/', 1565)\n",
    "#For MovieLens: The original dataset is unbalanced and one also needs to provide the desired ratio\n",
    "# of male/female graphs\n",
    "#female_ratio = 0.5\n",
    "#Vs, Es, classes = read_write_utilities.read_my_format('graphs/', 3400, female_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565 1565 1565\n"
     ]
    }
   ],
   "source": [
    "print(len(Vs), len(Es), len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after reading the graphs, we import the functions for the explicit feature maps generation\n",
    "import string_subtree_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the Count-Sketch data structure and the Tensor-Sketch that are used for the summarization of the polynomial\n",
    "#kernel of the k-gram distribution\n",
    "from count_sketch import CountSketch\n",
    "import tensorsketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_size = 1000 #the sketch size, b in the paper notation\n",
    "nr_tables = 1 #the number of hash tables per Count-Sketch, see the Count-Sketch paper for details\n",
    "max_p = 2 #the maximum degree of the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure there are at random files in the random folder\n",
    "random_files = 'random/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirac = False #setting Dirac=true results in collecting the neighborhood strings as features, i.e., the WL kernel\n",
    "h = 1 #the depth of the neighborhood subtrees\n",
    "k = 2 #the k in k-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count sketch data structures initialization\n",
      "Count-Sketch data structure initialized\n",
      "Count-Sketch data structure initialized\n",
      "Process graphs\n",
      "1565 1565 1565\n",
      "i =  0\n",
      "number of features 0\n",
      "i =  500\n",
      "number of features 70\n",
      "i =  1000\n",
      "number of features 76\n",
      "i =  1500\n",
      "number of features 82\n",
      "total number of features 82\n"
     ]
    }
   ],
   "source": [
    "vectors, vectors_cosine, dirac_vectors = string_subtree_kernels.graph2map(Vs, Es, len(classes), h, k, table_size, random_files, nr_tables, max_p, dirac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 1000\n"
     ]
    }
   ],
   "source": [
    "#the number of features of the original embedding and the sketches poly kernel embedding\n",
    "print(len(vectors[0][0]), len(vectors[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we write the feature maps into the folder feature_maps.  \n",
    "output = 'feature_maps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(h):\n",
    "        read_write_utilities.write_vectors_to_file(vectors[2*j], classes, output + str(j+1) + '_1_' + str(k) + '.txt')\n",
    "        read_write_utilities.write_vectors_to_file(vectors[2*j + 1], classes, output + str(j+1) + '_' + str(max_p) + '_' + str(k) + str(table_size) + '.txt')\n",
    "        read_write_utilities.write_vectors_to_file(vectors_cosine[2*j], classes, output + str(j+1) + '_1_cosine_' + str(k) + '.txt')\n",
    "        read_write_utilities.write_vectors_to_file(vectors_cosine[2*j + 1], classes, output + str(j+1) + '_' + str(max_p) +  '_cosine_' + str(k) + '_' + str(table_size) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1565, 82)\n"
     ]
    }
   ],
   "source": [
    "#Train Linear SVMs\n",
    "filename = 'feature_maps/1_1_cosine_2.txt'\n",
    "#read the data and convert to binary classification problem: class_label vs rest\n",
    "class_label = 6\n",
    "X, y = train_svm.read_data(filename, class_label)\n",
    "X = X.astype(float)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96815287 0.95541401 0.92993631 0.94904459 0.96815287 0.95512821\n",
      " 0.94871795 0.93589744 0.94230769 0.97435897]\n"
     ]
    }
   ],
   "source": [
    "#k-fold accuracy validation\n",
    "#Note that that in order to replicate the results from the paper one needs to generate 3400 graphs and set female_ratio=0.5. \n",
    "#For smaller number of graphs the results will be much worse\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "libsvm_clf = LinearSVC(penalty='l1', C= 1.0, loss='squared_hinge', dual=False, tol=1e-3)\n",
    "k_fold = KFold(n_splits=10)\n",
    "print(cross_val_score(libsvm_clf, X, y, cv=k_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scikitplot.metrics' has no attribute 'plot_roc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5ce4de6f5e94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_pred_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mskplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_roc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scikitplot.metrics' has no attribute 'plot_roc'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "samplesize = int(0.8*X.shape[0])\n",
    "X_train = X[:samplesize,:]\n",
    "y_train = y[:samplesize]   \n",
    "X_test = X[samplesize:, :]\n",
    "y_test = y[samplesize:] \n",
    "clf = CalibratedClassifierCV(libsvm_clf) \n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "skplt.metrics.plot_roc(y_test, y_pred_prob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
